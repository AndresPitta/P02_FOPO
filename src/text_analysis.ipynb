{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os.path\n",
    "import numpy as np\n",
    "import re\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import gensim \n",
    "from gensim.models import LdaModel\n",
    "from gensim.models.wrappers import LdaMallet\n",
    "\n",
    "import gensim.corpora as corpora\n",
    "from gensim.corpora import Dictionary\n",
    "\n",
    "from gensim import matutils, models\n",
    "\n",
    "import pyLDAvis.gensim\n",
    "import string\n",
    "import time\n",
    "import spacy\n",
    "# Load English model for SpaCy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "pd.set_option('display.max_colwidth', None) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = pd.read_csv(\"../processed_data/text_by_paragraph.csv\").drop(columns=['Unnamed: 0'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(text, \n",
    "               min_token_len = 2, \n",
    "               irrelevant_pos = ['PRON','CCONJ','PUNCT','PART','DET','ADP','SPACE', 'NUM', 'ADJ']): \n",
    "    \"\"\"\n",
    "    Given text, min_token_len, and irrelevant_pos carry out preprocessing of the text \n",
    "    and return a preprocessed string. \n",
    "    \n",
    "    Parameters\n",
    "    -------------\n",
    "    text : (str) \n",
    "        the text to be preprocessed\n",
    "    min_token_len : (int) \n",
    "        min_token_length required\n",
    "    irrelevant_pos : (list) \n",
    "        a list of irrelevant pos tags\n",
    "    \n",
    "    Returns\n",
    "    -------------\n",
    "    (str) the preprocessed text\n",
    "    \"\"\"\n",
    "    try:\n",
    "        doc = nlp(text)\n",
    "    except:\n",
    "        return \"missing value\"\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for token in doc:\n",
    "        \n",
    "        # Irrelevant POS\n",
    "        if token.pos_ in irrelevant_pos:\n",
    "            continue\n",
    "            \n",
    "        # Stop words\n",
    "        if token.is_stop:\n",
    "            continue\n",
    "        \n",
    "        # Word length    \n",
    "        if len(token)<2:\n",
    "            continue\n",
    "            \n",
    "            \n",
    "        results.append(token.lemma_)\n",
    "   \n",
    "    return \" \".join(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "text['Preprocessed_text'] = text['Corpus'].apply(preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [doc.split() for doc in text['Preprocessed_text'].tolist()]\n",
    "dictionary = corpora.Dictionary(corpus)\n",
    "dictionary.filter_extremes(no_below=10, no_above=0.1, keep_n= 100000)\n",
    "\n",
    "doc_term_matrix = [dictionary.doc2bow(doc) for doc in corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Mali'"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dictionary.get(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiments = {\n",
    "    'num_topics': [3, 10, 20],\n",
    "    'passes': [10, 20, 30, 40]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {\n",
    "    'model': [],\n",
    "    'model_name': [],\n",
    "    'running_time': []\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for j in experiments['num_topics']:\n",
    "    for k in experiments['passes']:\n",
    "        start = time.time()\n",
    "        model = models.LdaModel(corpus=doc_term_matrix, \n",
    "                                id2word=dictionary, \n",
    "                                num_topics=j, \n",
    "                                passes=k)\n",
    "        end = time.time() - start\n",
    "            \n",
    "        title = f'LDA using {j} topics and {k} passes'\n",
    "            \n",
    "        results['model'].append(model)\n",
    "        results['model_name'].append(title)\n",
    "        results['running_time'].append(end)\n",
    "            \n",
    "        print(title)\n",
    "        print('-------------------')\n",
    "        print(model.print_topics())\n",
    "        print('\\n')\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyLDAvis.enable_notebook()\n",
    "lda = results['model'][3]\n",
    "vis_wpn = pyLDAvis.gensim.prepare(lda, doc_term_matrix, dictionary, sort_topics=False)\n",
    "vis_wpn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_most_prob_topic(unseen_document, dictionary=dictionary, model = lda):\n",
    "    \"\"\"\n",
    "    Given an unseen_document, and a trained LDA model, this function\n",
    "    finds the most likely topic (topic with the highest probability) from the \n",
    "    topic distribution of the unseen document and returns the best topic with \n",
    "    its probability. . \n",
    "    \n",
    "    Parameters\n",
    "    ------------\n",
    "    unseen_document : (str) \n",
    "        the document to be labeled with a topic\n",
    "    \n",
    "    dictionary: (gensim Dictionary)\n",
    "        dictionary of the LDA model\n",
    "    \n",
    "    model : (gensim ldamodel) \n",
    "        the trained LDA model\n",
    "    \n",
    "    \n",
    "    Returns: \n",
    "    -------------\n",
    "        (str) a string of the form \n",
    "        `most likely topic label:probability of that label` \n",
    "    \n",
    "    Examples:\n",
    "    ----------\n",
    "    >> get_most_prob_topic(\"The research uses an HMM for discovering gene sequence.\", \n",
    "                            model = lda)\n",
    "    Science and Technology:0.435\n",
    "    \"\"\"  \n",
    "    unseen_doc_preprocessed = preprocess(unseen_document)\n",
    "    corpus_list = [unseen_doc_preprocessed.split()]\n",
    "    other_corpus = [dictionary.doc2bow(text) for text in corpus_list]\n",
    "    \n",
    "    results = model[other_corpus]\n",
    "    \n",
    "    topic = max(results[0],key=lambda item:item[1])\n",
    "    prob = 'Ambiguous' if topic[1] < 0.6 else 'Not Ambiguous'\n",
    "\n",
    "    return topic[0], prob\n",
    "\n",
    "def get_first(the_tuple):\n",
    "    '''get first element of the tuple (for the dataframe)\n",
    "    '''\n",
    "    return the_tuple[0]\n",
    "\n",
    "def get_second(the_tuple):\n",
    "    '''get second element of the tuple (for the dataframe)\n",
    "    '''\n",
    "    return the_tuple[1]\n",
    "\n",
    "def split_freq(text):\n",
    "    '''helper function for the frequency dataframe\n",
    "    '''\n",
    "    text_split = text.split(\"*\")\n",
    "    first, second = text_split[0], text_split[1].replace('\"','')\n",
    "    \n",
    "    return first, second"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "text['Topic'] = text['Preprocessed_text'].apply(get_most_prob_topic, args=(dictionary, lda,))\n",
    "text['Ambiguity'] = text['Topic'].apply(get_second)\n",
    "text['Topic'] = text['Topic'].apply(get_first)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "text.iloc[:,[0,7,8,10,9, 11]].to_csv(\"../processed_data/topics.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_freq = lda.show_topics(0,100)\n",
    "topic_res = {\n",
    "    'Topic': [],\n",
    "    'Words': []\n",
    "}\n",
    "\n",
    "for topic in topic_freq:\n",
    "    topic_res['Topic'].append(topic[0])\n",
    "    topic_res['Words'].append(topic[1].split(\"+\"))\n",
    "    \n",
    "topic_freq_df = pd.DataFrame(topic_res).explode(column = 'Words')\n",
    "topic_freq_df['Words'] = topic_freq_df['Words'].apply(split_freq)\n",
    "topic_freq_df['Freq'] = topic_freq_df['Words'].apply(get_first)\n",
    "topic_freq_df['Words'] = topic_freq_df['Words'].apply(get_second)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_freq_df.to_csv(\"../processed_data/frequency.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
