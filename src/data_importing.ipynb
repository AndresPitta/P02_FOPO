{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\apitt\\Anaconda3\\lib\\site-packages\\spacy\\util.py:275: UserWarning: [W031] Model 'en_core_web_sm' (2.2.5) requires spaCy v2.2 and is incompatible with the current spaCy version (2.3.5). This may lead to unexpected results or runtime errors. To resolve this, download a newer compatible model or retrain your custom model with the current spaCy version. For more details and available updates, run: python -m spacy validate\n",
      "  warnings.warn(warn_msg)\n"
     ]
    }
   ],
   "source": [
    "# Here we are using the Tika packages to import word documents\n",
    "# If you want to learn more, please go to: https://tika.apache.org/\n",
    "import tika\n",
    "\n",
    "from tika import parser\n",
    "\n",
    "# This are system packages for processing the data\n",
    "import re\n",
    "import os\n",
    "import datetime\n",
    "import io\n",
    "import string\n",
    "import time\n",
    "\n",
    "# Data handling\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# This package works for text processing\n",
    "import spacy\n",
    "# Load English model for SpaCy\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data importing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(text, \n",
    "               min_token_len = 2, \n",
    "               irrelevant_pos = ['PRON','CCONJ','PUNCT','PART','DET','ADP','SPACE']): \n",
    "    \"\"\"\n",
    "    Given text, min_token_len, and irrelevant_pos carry out preprocessing of the text \n",
    "    and return a preprocessed string. \n",
    "    \n",
    "    Parameters\n",
    "    -------------\n",
    "    text : (str) \n",
    "        the text to be preprocessed\n",
    "    min_token_len : (int) \n",
    "        min_token_length required\n",
    "    irrelevant_pos : (list) \n",
    "        a list of irrelevant pos tags\n",
    "    \n",
    "    Returns\n",
    "    -------------\n",
    "    (str) the preprocessed text\n",
    "    \"\"\"\n",
    "    try:\n",
    "        doc = nlp(text)\n",
    "    except:\n",
    "        return \"missing value\"\n",
    "    \n",
    "    results = []\n",
    "    counter = 0\n",
    "    \n",
    "    for token in doc:\n",
    "        \n",
    "        # Irrelevant POS\n",
    "        if token.pos_ in irrelevant_pos:\n",
    "            continue\n",
    "            \n",
    "        # Stop words\n",
    "        if token.is_stop:\n",
    "            continue\n",
    "        \n",
    "        # Word length    \n",
    "        if len(token)<2:\n",
    "            continue\n",
    "            \n",
    "        # Email\n",
    "        if token.like_email or token.like_url:\n",
    "            continue\n",
    "            \n",
    "        if counter>300:\n",
    "            break\n",
    "            \n",
    "        results.append(token.lemma_)\n",
    "        counter+=1\n",
    "   \n",
    "    if len(results)<=1:\n",
    "        final_text=\"no info\"\n",
    "    else:\n",
    "        final_text=\" \".join(results)\n",
    "    \n",
    "    return final_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_importing(directory, beginning):\n",
    "    '''\n",
    "    Imports the text data in a given folder, splits the text in paragraphs,\n",
    "    and returns a pandas dataset with the features\n",
    "    \n",
    "    Parameters:\n",
    "    ---------------------\n",
    "    directory (string):\n",
    "        root folder path\n",
    "    \n",
    "    beginning (string):\n",
    "        string that indicates the beginning of the text, if None the text starts at position 0.\n",
    "        \n",
    "    Returns:\n",
    "    ---------------------\n",
    "    pd.Dataframe:\n",
    "        imported text with features (document_name, Interviewee, Interviewer, Notetaker, Date, Considerations, Corpus)\n",
    "    '''\n",
    "    \n",
    "    dict_text = {\n",
    "        'Document_name': [],\n",
    "        'Interviewee': [],\n",
    "        'Interviewer': [],\n",
    "        'Notetaker': [],\n",
    "        'Date': [],\n",
    "        'Considerations':[],\n",
    "        'Corpus': []\n",
    "    }\n",
    "    \n",
    "    # Directory type handling\n",
    "    if not os.path.isdir(directory):\n",
    "        raise Exception(\"Directory does not exist\")\n",
    "    \n",
    "    # Iterating through the files of the directory\n",
    "    for subdirectory in os.listdir(directory):\n",
    "        folder_name = f'{directory}{subdirectory}/'\n",
    "       \n",
    "        for filename in os.listdir(folder_name):\n",
    "            file_data = parser.from_file(folder_name + filename)\n",
    "            text = file_data['content']\n",
    "\n",
    "            # Filling the features\n",
    "            dict_text['Document_name'].append(filename)\n",
    "\n",
    "            # Extracting the interviewee\n",
    "            interviewee = re.search('(Interviewee: )(.*)(\\\\n)', text)\n",
    "            if not interviewee is None: \n",
    "                dict_text['Interviewee'].append(interviewee.group(2))\n",
    "            else:\n",
    "                dict_text['Interviewee'].append(np.nan)\n",
    "\n",
    "            # Extracting the interviewer\n",
    "            interviewer = re.search('(Interviewer: )(.*)(\\\\n)', text)\n",
    "            if not interviewer is None: \n",
    "                dict_text['Interviewer'].append(interviewer.group(2))\n",
    "            else:\n",
    "                dict_text['Interviewer'].append(np.nan)\n",
    "\n",
    "            # Extracting the notetaker\n",
    "            notetaker = re.search('(Notetaker: )(.*)(\\\\n)', text)\n",
    "            if not notetaker is None: \n",
    "                dict_text['Notetaker'].append(notetaker.group(2))\n",
    "            else:\n",
    "                dict_text['Notetaker'].append(np.nan)\n",
    "\n",
    "            # Extracting the date\n",
    "            date = re.search('(Date: )(.*)(\\\\n)', text)\n",
    "            if not date is None: \n",
    "                try:\n",
    "                    date_obj = datetime.datetime.strptime(date.group(2), '%m/%d/%Y')\n",
    "                    date_obj = date_obj.date()\n",
    "                except:\n",
    "                    date_obj = date.group(2)\n",
    "\n",
    "                dict_text['Date'].append(date_obj)\n",
    "            else:\n",
    "                dict_text['Date'].append(np.nan)\n",
    "\n",
    "            # Extracting the notetaker\n",
    "            considerations = re.search('(Procedural considerations: )(.*)(\\\\n)', text)\n",
    "            if not considerations is None: \n",
    "                dict_text['Considerations'].append(considerations.group(2))\n",
    "            else:\n",
    "                dict_text['Considerations'].append(np.nan)\n",
    "\n",
    "            # Cropping the beginning of the text        \n",
    "            if beginning is not None:\n",
    "                text_start = re.search(beginning, text)\n",
    "\n",
    "                if not text_start is None: \n",
    "                    ts_index = text_start.span()[0]\n",
    "                    text_clean = text[ts_index:]\n",
    "                    text_for_dict = text_clean.split(\"\\n\")[9:]\n",
    "                else:\n",
    "                    text_for_dict = text.split(\"\\n\")            \n",
    "            else:\n",
    "                text_for_dict = text.split(\"\\n\")[9:]            \n",
    "\n",
    "            dict_text['Corpus'].append(list(enumerate(text_for_dict)))\n",
    "\n",
    "    return(pd.DataFrame(dict_text))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These 2 functions are made to extract the positions\n",
    "def extract_first(word_tuple):\n",
    "    first = word_tuple[0]\n",
    "    return first\n",
    "\n",
    "def extract_second(word_tuple):\n",
    "    second = word_tuple[1]\n",
    "    return second"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the path where the documents are, PLEASE CHANGE ACCORDINGLY\n",
    "text = text_importing(\"../data/\", \"Future of Peacekeeping â€“ Interviews\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_by_paragraph = text.explode('Corpus')\n",
    "text_by_paragraph['Order'] = text_by_paragraph['Corpus'].apply(extract_first)\n",
    "text_by_paragraph['Corpus'] = text_by_paragraph['Corpus'].apply(extract_second)\n",
    "text_by_paragraph['Length_corpus'] = text_by_paragraph['Corpus'].apply(len)\n",
    "text_by_paragraph['key'] = text_by_paragraph[['Document_name', 'Order']].apply(lambda x: '-'.join(x.astype(str)), axis=1)\n",
    "\n",
    "text_by_paragraph['Interviewee'] = text_by_paragraph['Interviewee'].fillna(\"No interviewee\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_by_paragraph.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_by_paragraph = text_by_paragraph.query('Length_corpus > 30')\n",
    "text_by_paragraph['Preprocessed_text'] = text_by_paragraph['Corpus'].apply(preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_by_paragraph.to_csv(\"../processed_data/text_by_paragraph.csv\")\n",
    "text.to_csv(\"../processed_data/text.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dict_importing(directory, beginning):\n",
    "    '''\n",
    "    Imports the dictionary of words to be used\n",
    "    and returns a pandas dataset with the features\n",
    "    \n",
    "    Parameters:\n",
    "    ---------------------\n",
    "    directory (string):\n",
    "        root folder path\n",
    "    \n",
    "    beginning (string):\n",
    "        string that indicates the beginning of the text, if None the text starts at position 0.\n",
    "    \n",
    "    Returns:\n",
    "    ---------------------\n",
    "    pd.Dataframe:\n",
    "        imported dictionary with features (word, group)\n",
    "    '''\n",
    "    \n",
    "    dict_text = {\n",
    "        'word': [],\n",
    "        'group': [],\n",
    "    }\n",
    "    \n",
    "    # Directory type handling\n",
    "    if not os.path.isdir(directory):\n",
    "        raise Exception(\"Directory does not exist\")\n",
    "        \n",
    "    # Iterating through the files of the directory\n",
    "    for filename in os.listdir(directory):\n",
    "        file_data = parser.from_file(directory + filename)\n",
    "        text = re.sub(\"[\\\\n\\\\t]\", \"\", file_data['content'])\n",
    "        text = re.sub(\"[\\\\xa0]\", \" \", text)\n",
    "        \n",
    "        if beginning is not None:\n",
    "            text_start = re.search(beginning, text)\n",
    "            \n",
    "            if not text_start is None: \n",
    "                ts_index = text_start.span()[0] + 2\n",
    "                text_clean = text[ts_index:]\n",
    "                text_for_dict = re.split(\"[0-9][0-9]?\\. \",text_clean)\n",
    "            else:\n",
    "                text_for_dict = re.split(\"[0-9][0-9]?\\. \",text)           \n",
    "        else:\n",
    "            text_for_dict = re.split(\"[0-9][0-9]?\\. \",text)\n",
    "  \n",
    "    for word in text_for_dict:\n",
    "        word_start = re.search(\"a\\.\", word)\n",
    "        \n",
    "        if word_start is None:\n",
    "            category=\"\"\n",
    "            subword_list=[]\n",
    "        else:\n",
    "            category = word[:word_start.span()[0]]\n",
    "            subword_list = re.split(\"[,] ?\",word[word_start.span()[0]+3:])\n",
    "        \n",
    "        dict_text['word'].append(category)\n",
    "        dict_text['group'].append(category)\n",
    "        \n",
    "        for sw in subword_list:\n",
    "            dict_text['word'].append(sw)\n",
    "            dict_text['group'].append(category)\n",
    "            \n",
    "    return pd.DataFrame(dict_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary_df = dict_importing(\"../dic/\", \"1.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary_df.to_csv(\"../processed_data/dictionary.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
